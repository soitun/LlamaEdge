searchState.loadedDescShard("llama_core", 0, "Llama Core, abbreviated as <code>llama-core</code>, defines a set of …\nThe directory for storing the archives in wasm virtual …\nBoth <code>text_to_image</code> and <code>image_to_image</code> contexts\n<code>image_to_image</code> context\nVersion info of the <code>wasi-nn_ggml</code> plugin, including the …\nRunning mode\nThe task type of the stable diffusion context\n<code>text_to_image</code> context\nDefine APIs for chat completion.\nDefine APIs for completions.\nDefine APIs for computing embeddings.\nError types for the Llama Core library.\nDefine APIs for file operations.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet the plugin info\nDefine Graph and GraphBuilder APIs for creating a new …\nDefine APIs for image generation and edit.\nInitialize the ggml context\nInitialize the ggml context for RAG scenarios.\nInitialize the piper context\nInitialize the stable-diffusion context with the given …\nInitialize the stable-diffusion context with the given …\nInitialize the whisper context\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDefine the types for model metadata.\nDefine APIs for querying models.\nDefine APIs for RAG operations.\nReturn the current running mode.\nDefine APIs for web search operations.\nDefine utility functions.\nTranscribe audio into the input language.\nTranslate audio into the target language\nGenerate audio from the input text.\nProcesses a chat-completion request and returns either a …\nProcesses a chat-completion request and returns a …\nProcesses a chat-completion request and returns …\nGiven a prompt, the model will return one or more …\nGenerate a list of chunks from a given text. Each chunk …\nGet the dimension of the embedding model.\nCompute embeddings for the given input.\nErrors thrown by the wasi-nn-ggml plugin and runtime.\nError types for wasi-nn errors.\nErrors in the model inference.\nErrors in the model inference in the stream mode.\nErrors in file not found.\nErrors in cleaning up the computation context in the …\nErrors in getting the output tensor.\nErrors in getting the output tensor in the stream mode.\nErrors in Context initialization.\nError types for the Llama Core library.\nErrors in General operation.\nErrors in Qdrant.\nErrors thrown by the Search Backend\nErrors in setting the input tensor.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDownload a specific file by id.\nList all files in the archives directory.\nRemove the target file by id.\nRetrieve information about a specific file by id.\nRetrieve the content of a specific file by id.\nEngine type\nWrapper of the <code>wasmedge_wasi_nn::Graph</code> struct\nBuilder for creating a new computation graph.\nGet the alias of the model\nCompute the inference on the given inputs.\nCompute the inference on the given inputs.\nClear the computation context.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCopy output tensor to out_buffer, return the output’s …\nCopy output tensor to out_buffer, return the output’s …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet the name of the model\nCreate a new computation graph builder.\nCreate a new computation graph from the given metadata.\nSet input uses the data, not only u8, but also f32, i32, …\nUpdate metadata\nCreate an edited or extended image given an original image …\nCreate an image given a prompt.\nCreate a variation of a given image.\nBase metadata trait\nDefine metadata for the ggml model.\nDefine metadata for the piper model.\nDefine metadata for the whisper model.\nMetadata for chat and embeddings models\nBuilder for creating a ggml metadata\nLogical maximum batch size. Defaults to 2048.\nSize of the prompt context. 0 means loaded from model. …\nRepeat alpha frequency penalty. Defaults to 0.0.\nReturns the argument unchanged.\nReturns the argument unchanged.\nBNF-like grammar to constrain generations (see samples in …\nPath to the image file for llava\nWhether to include usage in the stream response. Defaults …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nJSON schema to constrain generations (…\nThe main GPU to use. Defaults to None.\npath to the multimodal projector file for llava\nNumber of tokens to predict, -1 = infinity, -2 = until …\nRepeat alpha presence penalty. Defaults to 0.0.\nPenalize repeat sequence of tokens. Defaults to 1.0.\nHalt generation at PROMPT, return control in interactive …\nHow to split the model across multiple GPUs. Possible …\nAdjust the randomness of the generated text. Between 0.0 …\nHow split tensors should be distributed accross GPUs. If …\nNumber of threads to use during generation. Defaults to 2.\nTop-p sampling. Between 0.0 and 1.0. Defaults to 0.9.\nPhysical maximum batch size. Defaults to 512.\nWhether to use memory-mapped files for the model. Defaults …\nMetadata for chat and embeddings models\nBuilder for creating a ggml metadata\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nstdin input is lines of JSON instead of plain text. The …\nAmount of noise to add during audio generation. Defaults …\nVariation in phoneme lengths. Defaults to <code>0.8</code>.\nSeconds of silence after each sentence. Defaults to <code>0.2</code>.\nId of speaker. Defaults to <code>0</code>.\nThe speed of the generated audio. Select a value from <code>0.25</code> …\nThe sample rate of the audio input\nMetadata for whisper model\nBuilder for creating an audio metadata\nEnable debug mode. Defaults to false.\nAutomatically detect the spoken language in the provided …\nDuration of audio to process in milliseconds. Defaults to …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe language of the input audio. <code>auto</code> for auto-detection. …\nMaximum number of text context tokens to store. Defaults …\nMaximum segment length in characters. Defaults to 0.\nTime offset in milliseconds. Defaults to 0.\nOutput result in a CSV file. Defaults to false.\nOutput result in a JSON file. Defaults to false.\nOutput result in a lrc file. Defaults to false.\nOutput result in a srt file. Defaults to false.\nOutput result in a text file. Defaults to false.\nOutput result in a vtt file. Defaults to false.\nNumber of processors to use during computation. Defaults …\nText to guide the model. The max length is n_text_ctx/2 …\nSplit on word rather than on token. Defaults to false.\nSampling temperature, between 0 and 1. Defaults to 0.00.\nNumber of threads to use during computation. Defaults to 4.\nTranslate from source language to english. Defaults to …\nLists models available\nConvert document chunks to embeddings.\nConvert a query to embeddings.\nRetrieve similar points from the Qdrant server using the …\nPossible input/output Content Types. Currently only …\nThe base Search Configuration holding all relevant …\nFinal output format for consumption by the LLM.\noutput format for individual results in the final output.\nAdditional headers for any other purpose.\nThe content type of the input.\nThe endpoint for the search API.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe total number of results.\nMethod expected by the api endpoint.\nThe (expected) content type of the output.\nWrapper for the parser() function.\nCallback function to parse the output of the api-service. …\nPerform a web search with a <code>Serialize</code>-able input. The …\nThe search engine we’re currently focusing on. Currently …\nThe size limit of every search result.\nPrompts for use with summarization functionality. If set …\nContext size for summary generation. If <code>None</code>, will use the …\nPerform a search and summarize the corresponding search …\nReturn the names of the chat models.\nGet the chat prompt template type from the given model …\nReturn the names of the embedding models.")